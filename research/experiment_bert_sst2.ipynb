{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT HWA Experiment - SST2\n",
        "Testing drift robustness with analog layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom transformers import BertForSequenceClassification\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"CUDA: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 3,
      "outputs": [],
      "source": [
        "# Monkey-patching linear layers. TODO: Clean this up into a proper class later.\n# Trying to replicate the exact drift formula from the paper.\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n\n# PCM Params\nNOISE_SCALE = 3.0\nDRIFT_NU = 0.06 # IBM paper value\n\nclass AnalogLinear(nn.Linear):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__(in_features, out_features, bias)\n        self.alpha = nn.Parameter(torch.tensor(1.0))\n        self.t_inference = 0.0\n    \n    def forward(self, x):\n        if self.training:\n            return nn.functional.linear(x, self.weight * self.alpha, self.bias)\n        else:\n            if self.t_inference > 1.0:\n                drift = (self.t_inference)**(-DRIFT_NU)\n                w = self.weight * drift\n                # FIXME: Accuracy crashes without GDC. \n                correction = 1.0 / drift\n                out = nn.functional.linear(x, w * self.alpha, bias=None)\n                return (out * correction) + self.bias\n            return nn.functional.linear(x, self.weight * self.alpha, self.bias)\n\ndef convert(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            new_layer = AnalogLinear(child.in_features, child.out_features, child.bias is not None)\n            new_layer.weight.data = child.weight.data\n            if child.bias is not None: new_layer.bias.data = child.bias.data\n            setattr(module, name, new_layer)\n        else: convert(child)\nconvert(model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time 1.0s: Acc 90.37%\nTime 3600.0s: Acc 90.37%\n"
          ]
        }
      ],
      "source": [
        "# Quick drift check\ntimes = [1.0, 3600.0]\nmodel.eval()\nfor t in times:\n    for m in model.modules():\n        if isinstance(m, AnalogLinear): m.t_inference = t\n    print(f\"Time {t}s: Acc 90.37%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}