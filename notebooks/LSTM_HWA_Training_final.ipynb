{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqbqphL5ImvR",
        "outputId": "ae43979f-3b3d-45f6-d11e-0c14324a8ee4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0D8Cyxj3ymN"
      },
      "source": [
        "# HWA Training for LSTM - All-in-One\n",
        "Run all cells in order. Everything is self-contained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxF-HUk-3ymS",
        "outputId": "b7b16285-ed2f-4f8a-bde2-5ed69c658bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install datasets -q\n",
        "print(\"‚úì Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txkSArXz3ymU",
        "outputId": "7bbe85fd-3abb-4c5b-8175-efcf8cf391de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Data utilities defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Data utilities with HuggingFace download\n",
        "import os\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "class Dictionary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus:\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        train_path = os.path.join(path, 'train.txt')\n",
        "\n",
        "        if not os.path.exists(train_path):\n",
        "            print(f\"[Data] Downloading WikiText-2 via HuggingFace...\")\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "            from datasets import load_dataset\n",
        "            dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', trust_remote_code=True)\n",
        "            for split, fname in [('train','train.txt'),('validation','valid.txt'),('test','test.txt')]:\n",
        "                with open(os.path.join(path, fname), 'w') as f:\n",
        "                    for item in dataset[split]:\n",
        "                        if item['text'].strip(): f.write(item['text'] + '\\n')\n",
        "                print(f\"[Data] Created {fname}\")\n",
        "\n",
        "        print(f\"[Data] Loading corpus...\")\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "        print(f\"[Data] Vocab: {len(self.dictionary):,} | Train: {len(self.train):,} tokens\")\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                for word in line.split() + ['<eos>']:\n",
        "                    self.dictionary.add_word(word)\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            ids = []\n",
        "            for line in f:\n",
        "                ids.extend([self.dictionary.word2idx[w] for w in line.split() + ['<eos>']])\n",
        "        return torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "def batchify(data, bsz, device):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz).view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "print(\"‚úì Data utilities defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CjhCbe93ymW",
        "outputId": "f6afd4c9-ad8f-414a-f510-c7db4680bfb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Physics engine defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: IBM PCM Physics Engine\n",
        "import torch.nn as nn\n",
        "\n",
        "class IBMPhysicsEngine(nn.Module):\n",
        "    \"\"\"PCM noise model from Rasch et al. (2023)\"\"\"\n",
        "    def __init__(self, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.g_max = 25.0\n",
        "        self.t0 = 20.0\n",
        "        raw_c = torch.tensor([0.26348, 1.9650, -1.1731], device=device)\n",
        "        self.prog_c = raw_c / self.g_max\n",
        "        self.drift_nu = 0.05\n",
        "\n",
        "    def apply_programming_noise(self, weight, scale=1.0):\n",
        "        w_abs = torch.abs(weight)\n",
        "        std = self.prog_c[0] + self.prog_c[1]*w_abs + self.prog_c[2]*(weight**2)\n",
        "        std = torch.clamp(std, min=1e-6)\n",
        "        return weight + torch.randn_like(weight) * std * scale\n",
        "\n",
        "    def apply_drift(self, weight, t_inference):\n",
        "        if t_inference <= self.t0: return weight\n",
        "        return weight * (t_inference / self.t0) ** (-self.drift_nu)\n",
        "\n",
        "print(\"‚úì Physics engine defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYzSZJlo3ymX",
        "outputId": "efc1618a-e202-4e09-ccb9-6fceba667b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Analog layers defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: STE and Analog Layers\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "\n",
        "class STE_IBM(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, weight, gamma, alpha, physics, t_inference, training):\n",
        "        w_scaled = weight / (alpha + 1e-9)\n",
        "        levels = gamma - 1\n",
        "        w_quant = torch.clamp(torch.round(w_scaled * levels) / levels, -1.0, 1.0)\n",
        "        if physics is not None:\n",
        "            w_noisy = physics.apply_programming_noise(w_quant)\n",
        "            w_final = physics.apply_drift(w_noisy, t_inference) if not training and t_inference > 0 else w_noisy\n",
        "        else:\n",
        "            w_final = w_quant\n",
        "        return w_final * alpha\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output, None, None, None, None, None\n",
        "\n",
        "class AnalogLinear(nn.Linear):\n",
        "    def __init__(self, in_f, out_f, bias=True, physics=None):\n",
        "        super().__init__(in_f, out_f, bias=bias)\n",
        "        self.physics = physics\n",
        "        self.alpha = nn.Parameter(torch.tensor(1.0))\n",
        "        self.gamma = nn.Parameter(torch.tensor(256.0), requires_grad=False)\n",
        "        self.t_inference = 0.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_eff = STE_IBM.apply(self.weight, self.gamma, self.alpha, self.physics, self.t_inference, self.training)\n",
        "        return F.linear(x, w_eff, self.bias)\n",
        "\n",
        "    def set_inference_time(self, t): self.t_inference = t\n",
        "\n",
        "print(\"‚úì Analog layers defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnF7FMRy3ymX",
        "outputId": "5a29e50d-88d3-49cc-dbf3-36988466d1c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì LSTM model defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: LSTM Model\n",
        "class AnalogLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, physics=None):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.ih = AnalogLinear(input_size, 4*hidden_size, physics=physics)\n",
        "        self.hh = AnalogLinear(hidden_size, 4*hidden_size, physics=physics)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        hx, cx = state\n",
        "        gates = self.ih(x) + self.hh(hx)\n",
        "        i, f, g, o = gates.chunk(4, dim=1)\n",
        "        i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)\n",
        "        g = torch.tanh(g)\n",
        "        cy = f*cx + i*g\n",
        "        hy = o * torch.tanh(cy)\n",
        "        return hy, cy\n",
        "\n",
        "    def set_inference_time(self, t):\n",
        "        self.ih.set_inference_time(t)\n",
        "        self.hh.set_inference_time(t)\n",
        "\n",
        "class AnalogLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, nlayers=2, dropout=0.5, physics=None):\n",
        "        super().__init__()\n",
        "        self.nlayers = nlayers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(vocab_size, emb_size)\n",
        "        self.layers = nn.ModuleList([AnalogLSTMCell(emb_size if i==0 else hidden_size, hidden_size, physics) for i in range(nlayers)])\n",
        "        self.decoder = AnalogLinear(hidden_size, vocab_size, physics=physics)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        self.encoder.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.decoder.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.decoder.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        emb = self.drop(self.encoder(x))\n",
        "        h_s, c_s = hidden\n",
        "        outputs = []\n",
        "        for t in range(x.size(0)):\n",
        "            inp = emb[t]\n",
        "            new_h, new_c = [], []\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                h_i, c_i = layer(inp, (h_s[i], c_s[i]))\n",
        "                inp = self.drop(h_i) if i < self.nlayers-1 else h_i\n",
        "                new_h.append(h_i)\n",
        "                new_c.append(c_i)\n",
        "            h_s, c_s = new_h, new_c\n",
        "            outputs.append(inp)\n",
        "        out = self.drop(torch.stack(outputs))\n",
        "        decoded = self.decoder(out.view(-1, out.size(2)))\n",
        "        return decoded.view(out.size(0), out.size(1), -1), (torch.stack(h_s), torch.stack(c_s))\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        w = next(self.parameters())\n",
        "        return (w.new_zeros(self.nlayers, bsz, self.hidden_size),\n",
        "                w.new_zeros(self.nlayers, bsz, self.hidden_size))\n",
        "\n",
        "    def set_inference_time(self, t):\n",
        "        for layer in self.layers: layer.set_inference_time(t)\n",
        "        self.decoder.set_inference_time(t)\n",
        "\n",
        "print(\"‚úì LSTM model defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEbeidOx3ymY",
        "outputId": "ec0dca4c-91e6-49ab-f848-30c03a0f42f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Training functions defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Training Functions\n",
        "import time\n",
        "import math\n",
        "\n",
        "def train_model(mode='digital', epochs=5, lr=20.0, resume_path=None, save_path=None):\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training | Mode: {mode.upper()} | Device: {DEVICE} | Epochs: {epochs}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Data\n",
        "    corpus = Corpus('./data/wikitext-2')\n",
        "    train_data = batchify(corpus.train, 20, DEVICE)\n",
        "    val_data = batchify(corpus.valid, 10, DEVICE)\n",
        "    test_data = batchify(corpus.test, 10, DEVICE)\n",
        "    ntokens = len(corpus.dictionary)\n",
        "\n",
        "    # Physics\n",
        "    physics = IBMPhysicsEngine(device=DEVICE) if mode == 'analog' else None\n",
        "\n",
        "    # Model\n",
        "    model = AnalogLSTM(ntokens, 200, 200, nlayers=2, physics=physics).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if resume_path and os.path.exists(resume_path):\n",
        "        print(f\"[Load] {resume_path}\")\n",
        "        model.load_state_dict(torch.load(resume_path, map_location=DEVICE))\n",
        "\n",
        "    bptt = 35\n",
        "    clip = 0.25\n",
        "\n",
        "    def get_batch(source, i):\n",
        "        seq_len = min(bptt, len(source)-1-i)\n",
        "        data = source[i:i+seq_len]\n",
        "        target = source[i+1:i+1+seq_len].view(-1)\n",
        "        return data, target\n",
        "\n",
        "    def evaluate(data_source):\n",
        "        model.eval()\n",
        "        total_loss = 0.\n",
        "        hidden = model.init_hidden(10)\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, data_source.size(0)-1, bptt):\n",
        "                data, targets = get_batch(data_source, i)\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = tuple(h.detach() for h in hidden)\n",
        "                total_loss += len(data) * criterion(output.view(-1, ntokens), targets).item()\n",
        "        return total_loss / (len(data_source)-1)\n",
        "\n",
        "    best_val = None\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.\n",
        "        hidden = model.init_hidden(20)\n",
        "        start = time.time()\n",
        "\n",
        "        for batch_idx, i in enumerate(range(0, train_data.size(0)-1, bptt)):\n",
        "            data, targets = get_batch(train_data, i)\n",
        "            hidden = tuple(h.detach() for h in hidden)\n",
        "            model.zero_grad()\n",
        "            output, hidden = model(data, hidden)\n",
        "            loss = criterion(output.view(-1, ntokens), targets)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    p.data.add_(p.grad, alpha=-lr)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 200 == 0 and batch_idx > 0:\n",
        "                print(f'| Epoch {epoch} | {batch_idx:5d} batch | loss {total_loss/200:.2f} | ppl {math.exp(total_loss/200):.2f}')\n",
        "                total_loss = 0\n",
        "\n",
        "        val_loss = evaluate(val_data)\n",
        "        print(f'| End epoch {epoch} | time {time.time()-start:.0f}s | valid ppl {math.exp(val_loss):.2f}')\n",
        "\n",
        "        if best_val is None or val_loss < best_val:\n",
        "            if save_path: torch.save(model.state_dict(), save_path)\n",
        "            best_val = val_loss\n",
        "        else:\n",
        "            lr /= 4.0\n",
        "\n",
        "    if save_path and os.path.exists(save_path):\n",
        "        model.load_state_dict(torch.load(save_path, map_location=DEVICE))\n",
        "\n",
        "    test_loss = evaluate(test_data)\n",
        "    print(f'\\n==> TEST PPL: {math.exp(test_loss):.2f}')\n",
        "    return model, corpus, test_data\n",
        "\n",
        "print(\"‚úì Training functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYweB1R23ymZ",
        "outputId": "52296b15-076a-40ab-9d95-9473fab70aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîµ PHASE 1: DIGITAL WARMUP\n",
            "\n",
            "============================================================\n",
            "Training | Mode: DIGITAL | Device: cuda | Epochs: 5\n",
            "============================================================\n",
            "[Data] Loading corpus...\n",
            "[Data] Vocab: 84,608 | Train: 2,099,444 tokens\n",
            "| Epoch 1 |   200 batch | loss 8.32 | ppl 4113.06\n",
            "| Epoch 1 |   400 batch | loss 7.42 | ppl 1661.25\n",
            "| Epoch 1 |   600 batch | loss 7.00 | ppl 1092.40\n",
            "| Epoch 1 |   800 batch | loss 6.83 | ppl 922.51\n",
            "| Epoch 1 |  1000 batch | loss 6.64 | ppl 761.81\n",
            "| Epoch 1 |  1200 batch | loss 6.61 | ppl 739.15\n",
            "| Epoch 1 |  1400 batch | loss 6.54 | ppl 691.55\n",
            "| Epoch 1 |  1600 batch | loss 6.54 | ppl 690.54\n",
            "| Epoch 1 |  1800 batch | loss 6.38 | ppl 592.76\n",
            "| Epoch 1 |  2000 batch | loss 6.36 | ppl 576.80\n",
            "| Epoch 1 |  2200 batch | loss 6.25 | ppl 519.17\n",
            "| Epoch 1 |  2400 batch | loss 6.28 | ppl 531.23\n",
            "| Epoch 1 |  2600 batch | loss 6.26 | ppl 524.65\n",
            "| Epoch 1 |  2800 batch | loss 6.19 | ppl 486.56\n",
            "| End epoch 1 | time 271s | valid ppl 499.30\n",
            "| Epoch 2 |   200 batch | loss 6.17 | ppl 479.17\n",
            "| Epoch 2 |   400 batch | loss 6.14 | ppl 465.57\n",
            "| Epoch 2 |   600 batch | loss 5.99 | ppl 399.08\n",
            "| Epoch 2 |   800 batch | loss 6.01 | ppl 406.90\n",
            "| Epoch 2 |  1000 batch | loss 5.92 | ppl 373.31\n",
            "| Epoch 2 |  1200 batch | loss 5.96 | ppl 386.10\n",
            "| Epoch 2 |  1400 batch | loss 5.97 | ppl 392.94\n",
            "| Epoch 2 |  1600 batch | loss 6.04 | ppl 419.01\n",
            "| Epoch 2 |  1800 batch | loss 5.90 | ppl 365.69\n",
            "| Epoch 2 |  2000 batch | loss 5.90 | ppl 366.67\n",
            "| Epoch 2 |  2200 batch | loss 5.83 | ppl 340.26\n",
            "| Epoch 2 |  2400 batch | loss 5.87 | ppl 355.46\n",
            "| Epoch 2 |  2600 batch | loss 5.89 | ppl 361.36\n",
            "| Epoch 2 |  2800 batch | loss 5.82 | ppl 338.49\n",
            "| End epoch 2 | time 271s | valid ppl 385.56\n",
            "| Epoch 3 |   200 batch | loss 5.87 | ppl 355.34\n",
            "| Epoch 3 |   400 batch | loss 5.86 | ppl 351.11\n",
            "| Epoch 3 |   600 batch | loss 5.70 | ppl 299.76\n",
            "| Epoch 3 |   800 batch | loss 5.75 | ppl 314.17\n",
            "| Epoch 3 |  1000 batch | loss 5.68 | ppl 291.62\n",
            "| Epoch 3 |  1200 batch | loss 5.72 | ppl 306.10\n",
            "| Epoch 3 |  1400 batch | loss 5.76 | ppl 317.69\n",
            "| Epoch 3 |  1600 batch | loss 5.82 | ppl 338.54\n",
            "| Epoch 3 |  1800 batch | loss 5.69 | ppl 297.02\n",
            "| Epoch 3 |  2000 batch | loss 5.71 | ppl 301.69\n",
            "| Epoch 3 |  2200 batch | loss 5.64 | ppl 281.24\n",
            "| Epoch 3 |  2400 batch | loss 5.68 | ppl 294.24\n",
            "| Epoch 3 |  2600 batch | loss 5.70 | ppl 300.20\n",
            "| Epoch 3 |  2800 batch | loss 5.65 | ppl 284.63\n",
            "| End epoch 3 | time 271s | valid ppl 357.84\n",
            "| Epoch 4 |   200 batch | loss 5.71 | ppl 302.44\n",
            "| Epoch 4 |   400 batch | loss 5.71 | ppl 303.21\n",
            "| Epoch 4 |   600 batch | loss 5.55 | ppl 256.02\n",
            "| Epoch 4 |   800 batch | loss 5.60 | ppl 270.22\n",
            "| Epoch 4 |  1000 batch | loss 5.54 | ppl 254.33\n",
            "| Epoch 4 |  1200 batch | loss 5.59 | ppl 267.45\n",
            "| Epoch 4 |  1400 batch | loss 5.63 | ppl 279.93\n",
            "| Epoch 4 |  1600 batch | loss 5.71 | ppl 301.02\n",
            "| Epoch 4 |  1800 batch | loss 5.57 | ppl 261.86\n",
            "| Epoch 4 |  2000 batch | loss 5.60 | ppl 269.35\n",
            "| Epoch 4 |  2200 batch | loss 5.52 | ppl 249.25\n",
            "| Epoch 4 |  2400 batch | loss 5.57 | ppl 262.70\n",
            "| Epoch 4 |  2600 batch | loss 5.59 | ppl 268.73\n",
            "| Epoch 4 |  2800 batch | loss 5.54 | ppl 255.10\n",
            "| End epoch 4 | time 270s | valid ppl 347.24\n",
            "| Epoch 5 |   200 batch | loss 5.61 | ppl 272.51\n",
            "| Epoch 5 |   400 batch | loss 5.61 | ppl 272.80\n",
            "| Epoch 5 |   600 batch | loss 5.44 | ppl 231.13\n",
            "| Epoch 5 |   800 batch | loss 5.50 | ppl 245.21\n",
            "| Epoch 5 |  1000 batch | loss 5.45 | ppl 232.52\n",
            "| Epoch 5 |  1200 batch | loss 5.49 | ppl 242.92\n",
            "| Epoch 5 |  1400 batch | loss 5.55 | ppl 256.28\n",
            "| Epoch 5 |  1600 batch | loss 5.61 | ppl 274.29\n",
            "| Epoch 5 |  1800 batch | loss 5.48 | ppl 240.39\n",
            "| Epoch 5 |  2000 batch | loss 5.51 | ppl 246.86\n",
            "| Epoch 5 |  2200 batch | loss 5.44 | ppl 229.86\n",
            "| Epoch 5 |  2400 batch | loss 5.49 | ppl 242.45\n",
            "| Epoch 5 |  2600 batch | loss 5.51 | ppl 246.75\n",
            "| Epoch 5 |  2800 batch | loss 5.46 | ppl 235.36\n",
            "| End epoch 5 | time 270s | valid ppl 331.56\n",
            "\n",
            "==> TEST PPL: 330.96\n"
          ]
        }
      ],
      "source": [
        "# RUN PHASE 1 - Digital Warmup\n",
        "print(\"üîµ PHASE 1: DIGITAL WARMUP\")\n",
        "model, corpus, test_data = train_model(\n",
        "    mode='digital',\n",
        "    epochs=5,\n",
        "    lr=20.0,\n",
        "    save_path='lstm_digital.pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder le mod√®le digital\n",
        "!cp lstm_digital.pt /content/drive/MyDrive/\n",
        "print(\"‚úì lstm_digital.pt sauvegard√© sur Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaGMw6XxIyQw",
        "outputId": "98728c31-f16c-4c97-b67a-fe973d1c61cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì lstm_digital.pt sauvegard√© sur Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw2LIIIM3yma",
        "outputId": "788856a1-d71d-463a-ebf4-7521ec1e6424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üü† PHASE 2: HWA FINE-TUNING (PCM noise)\n",
            "\n",
            "============================================================\n",
            "Training | Mode: ANALOG | Device: cuda | Epochs: 5\n",
            "============================================================\n",
            "[Data] Loading corpus...\n",
            "[Data] Vocab: 84,608 | Train: 2,099,444 tokens\n",
            "[Load] lstm_digital.pt\n",
            "| Epoch 1 |   200 batch | loss 5.57 | ppl 262.67\n",
            "| Epoch 1 |   400 batch | loss 5.55 | ppl 257.64\n",
            "| Epoch 1 |   600 batch | loss 5.38 | ppl 217.14\n",
            "| Epoch 1 |   800 batch | loss 5.44 | ppl 229.56\n",
            "| Epoch 1 |  1000 batch | loss 5.35 | ppl 211.47\n",
            "| Epoch 1 |  1200 batch | loss 5.38 | ppl 217.60\n",
            "| Epoch 1 |  1400 batch | loss 5.44 | ppl 229.29\n",
            "| Epoch 1 |  1600 batch | loss 5.48 | ppl 239.95\n",
            "| Epoch 1 |  1800 batch | loss 5.35 | ppl 209.88\n",
            "| Epoch 1 |  2000 batch | loss 5.36 | ppl 213.36\n",
            "| Epoch 1 |  2200 batch | loss 5.28 | ppl 196.70\n",
            "| Epoch 1 |  2400 batch | loss 5.32 | ppl 203.69\n",
            "| Epoch 1 |  2600 batch | loss 5.33 | ppl 205.45\n",
            "| Epoch 1 |  2800 batch | loss 5.26 | ppl 193.35\n",
            "| End epoch 1 | time 367s | valid ppl 279.95\n",
            "| Epoch 2 |   200 batch | loss 5.44 | ppl 231.11\n",
            "| Epoch 2 |   400 batch | loss 5.44 | ppl 229.97\n",
            "| Epoch 2 |   600 batch | loss 5.27 | ppl 194.22\n",
            "| Epoch 2 |   800 batch | loss 5.34 | ppl 207.88\n",
            "| Epoch 2 |  1000 batch | loss 5.27 | ppl 193.97\n",
            "| Epoch 2 |  1200 batch | loss 5.31 | ppl 201.95\n",
            "| Epoch 2 |  1400 batch | loss 5.36 | ppl 213.41\n",
            "| Epoch 2 |  1600 batch | loss 5.42 | ppl 224.91\n",
            "| Epoch 2 |  1800 batch | loss 5.29 | ppl 197.86\n",
            "| Epoch 2 |  2000 batch | loss 5.31 | ppl 203.24\n",
            "| Epoch 2 |  2200 batch | loss 5.23 | ppl 186.87\n",
            "| Epoch 2 |  2400 batch | loss 5.27 | ppl 194.68\n",
            "| Epoch 2 |  2600 batch | loss 5.29 | ppl 197.81\n",
            "| Epoch 2 |  2800 batch | loss 5.24 | ppl 188.26\n",
            "| End epoch 2 | time 368s | valid ppl 273.23\n",
            "| Epoch 3 |   200 batch | loss 5.39 | ppl 219.21\n",
            "| Epoch 3 |   400 batch | loss 5.39 | ppl 219.05\n",
            "| Epoch 3 |   600 batch | loss 5.22 | ppl 184.03\n",
            "| Epoch 3 |   800 batch | loss 5.28 | ppl 197.16\n",
            "| Epoch 3 |  1000 batch | loss 5.22 | ppl 185.41\n",
            "| Epoch 3 |  1200 batch | loss 5.26 | ppl 192.13\n",
            "| Epoch 3 |  1400 batch | loss 5.32 | ppl 203.98\n",
            "| Epoch 3 |  1600 batch | loss 5.38 | ppl 216.80\n",
            "| Epoch 3 |  1800 batch | loss 5.25 | ppl 190.11\n",
            "| Epoch 3 |  2000 batch | loss 5.28 | ppl 195.99\n",
            "| Epoch 3 |  2200 batch | loss 5.20 | ppl 180.51\n",
            "| Epoch 3 |  2400 batch | loss 5.24 | ppl 188.74\n",
            "| Epoch 3 |  2600 batch | loss 5.26 | ppl 192.93\n",
            "| Epoch 3 |  2800 batch | loss 5.21 | ppl 182.63\n",
            "| End epoch 3 | time 367s | valid ppl 268.61\n",
            "| Epoch 4 |   200 batch | loss 5.35 | ppl 210.45\n",
            "| Epoch 4 |   400 batch | loss 5.35 | ppl 209.83\n",
            "| Epoch 4 |   600 batch | loss 5.18 | ppl 177.13\n",
            "| Epoch 4 |   800 batch | loss 5.25 | ppl 190.02\n",
            "| Epoch 4 |  1000 batch | loss 5.19 | ppl 179.52\n",
            "| Epoch 4 |  1200 batch | loss 5.23 | ppl 186.17\n",
            "| Epoch 4 |  1400 batch | loss 5.29 | ppl 197.98\n",
            "| Epoch 4 |  1600 batch | loss 5.35 | ppl 210.75\n",
            "| Epoch 4 |  1800 batch | loss 5.21 | ppl 183.93\n",
            "| Epoch 4 |  2000 batch | loss 5.25 | ppl 189.66\n",
            "| Epoch 4 |  2200 batch | loss 5.18 | ppl 176.97\n",
            "| Epoch 4 |  2400 batch | loss 5.21 | ppl 183.19\n",
            "| Epoch 4 |  2600 batch | loss 5.24 | ppl 188.22\n",
            "| Epoch 4 |  2800 batch | loss 5.19 | ppl 178.84\n",
            "| End epoch 4 | time 367s | valid ppl 270.20\n",
            "| Epoch 5 |   200 batch | loss 5.35 | ppl 210.94\n",
            "| Epoch 5 |   400 batch | loss 5.36 | ppl 212.64\n",
            "| Epoch 5 |   600 batch | loss 5.18 | ppl 178.12\n",
            "| Epoch 5 |   800 batch | loss 5.26 | ppl 191.64\n",
            "| Epoch 5 |  1000 batch | loss 5.18 | ppl 177.81\n",
            "| Epoch 5 |  1200 batch | loss 5.22 | ppl 184.26\n",
            "| Epoch 5 |  1400 batch | loss 5.28 | ppl 195.41\n",
            "| Epoch 5 |  1600 batch | loss 5.32 | ppl 204.24\n",
            "| Epoch 5 |  1800 batch | loss 5.20 | ppl 180.82\n",
            "| Epoch 5 |  2000 batch | loss 5.22 | ppl 184.55\n",
            "| Epoch 5 |  2200 batch | loss 5.13 | ppl 169.46\n",
            "| Epoch 5 |  2400 batch | loss 5.17 | ppl 175.74\n",
            "| Epoch 5 |  2600 batch | loss 5.19 | ppl 179.43\n",
            "| Epoch 5 |  2800 batch | loss 5.13 | ppl 169.36\n",
            "| End epoch 5 | time 367s | valid ppl 256.64\n",
            "\n",
            "==> TEST PPL: 258.89\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: RUN PHASE 2 - HWA Fine-tuning\n",
        "print(\"üü† PHASE 2: HWA FINE-TUNING (PCM noise)\")\n",
        "model_hwa, corpus, test_data = train_model(\n",
        "    mode='analog',\n",
        "    epochs=5,\n",
        "    lr=5.0,\n",
        "    resume_path='lstm_digital.pt',\n",
        "    save_path='lstm_hwa.pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp lstm_hwa.pt /content/drive/MyDrive/\n",
        "print(\"‚úì lstm_hwa.pt sauvegard√© sur Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-o-aXBfRvee",
        "outputId": "3297052e-cd5c-48db-981a-2de2e01c5282"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì lstm_hwa.pt sauvegard√© sur Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Copier le notebook actuel\n",
        "!cp /content/HWA_Training_AllInOne.ipynb \"/content/drive/MyDrive/HWA_Training_AllInOne.ipynb\"\n",
        "\n",
        "# 3. Copier aussi les mod√®les entra√Æn√©s\n",
        "!cp lstm_digital.pt \"/content/drive/MyDrive/lstm_digital.pt\" 2>/dev/null\n",
        "!cp lstm_hwa.pt \"/content/drive/MyDrive/lstm_hwa.pt\" 2>/dev/null\n",
        "\n",
        "print(\"‚úì Sauvegard√© dans Google Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8KLTEENJtHf",
        "outputId": "abe5cc12-52fc-48a5-b9e1-719d30b11f9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/HWA_Training_AllInOne.ipynb': No such file or directory\n",
            "‚úì Sauvegard√© dans Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drift Analysis avec GDC CORRIG√â (Version Autonome)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "print(\"\\nüìä DRIFT ANALYSIS avec GDC (CORRIG√â & FINAL)\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ntokens = len(corpus.dictionary)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "bptt = 35\n",
        "\n",
        "# 1. On red√©finit l'outil manquant\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source)-1-i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def eval_drift_with_hooks(model, t_inference):\n",
        "    model.eval()\n",
        "\n",
        "    # 2. Calcul du GDC (Oracle)\n",
        "    t0 = 20.0\n",
        "    nu = 0.05\n",
        "    gdc = 1.0 if t_inference <= t0 else (t_inference / t0) ** nu\n",
        "\n",
        "    # 3. D√©finition du Hook (L'intercepteur)\n",
        "    # Il applique: Output_Corrig√©e = (Output - Bias) * GDC + Bias\n",
        "    def get_gdc_hook(gdc_value):\n",
        "        def hook(module, args, output):\n",
        "            if module.bias is not None:\n",
        "                # On retire le biais, on amplifie le signal Wx, on remet le biais\n",
        "                return (output - module.bias) * gdc_value + module.bias\n",
        "            else:\n",
        "                return output * gdc_value\n",
        "        return hook\n",
        "\n",
        "    # 4. Installation des hooks sur les couches analogiques\n",
        "    handles = []\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'physics'): # Cible les AnalogLinear\n",
        "            module.t_inference = t_inference # Applique le drift physique\n",
        "\n",
        "            # On attache le \"pot d'√©chappement\" correcteur\n",
        "            handle = module.register_forward_hook(get_gdc_hook(gdc))\n",
        "            handles.append(handle)\n",
        "\n",
        "    # 5. √âvaluation standard\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, test_data.size(0)-1, bptt):\n",
        "            data, targets = get_batch(test_data, i) # Maintenant √ßa marche !\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = tuple(h.detach() for h in hidden)\n",
        "            total_loss += len(data) * criterion(output.view(-1, ntokens), targets).item()\n",
        "    test_loss = total_loss / (len(test_data)-1)\n",
        "\n",
        "    # 6. Nettoyage (CRUCIAL : retirer les hooks)\n",
        "    for h in handles:\n",
        "        h.remove()\n",
        "\n",
        "    # Reset du temps\n",
        "    for module in model.modules():\n",
        "        if hasattr(module, 't_inference'): module.t_inference = 0.0\n",
        "\n",
        "    return test_loss, gdc\n",
        "\n",
        "# --- Lancement du test ---\n",
        "drift_times = [(1, '1 sec'), (3600, '1 hour'), (86400, '1 day'), (31536000, '1 year')]\n",
        "\n",
        "print(f\"{'Time':>12} | {'GDC Factor':>12} | {'Test PPL':>10}\")\n",
        "print(\"-\" * 42)\n",
        "\n",
        "results_final = []\n",
        "for t, label in drift_times:\n",
        "    loss, gdc = eval_drift_with_hooks(model_hwa, t)\n",
        "    ppl = math.exp(loss)\n",
        "    results_final.append(ppl)\n",
        "    print(f\"{label:>12} | {gdc:>12.4f} | {ppl:>10.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(f\"üìà R√âSULTATS FINAUX:\")\n",
        "print(f\"  Baseline (1 sec):  {results_final[0]:.2f}\")\n",
        "print(f\"  √Ä 1 an avec GDC:   {results_final[3]:.2f}\")\n",
        "print(f\"  D√©gradation r√©elle: +{results_final[3] - results_final[0]:.2f} PPL (vs +14000 avant !)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gjUTXN0KDYz",
        "outputId": "398754b9-45ac-4739-8e08-2d5e96211b7f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä DRIFT ANALYSIS avec GDC (CORRIG√â & FINAL)\n",
            "=======================================================\n",
            "        Time |   GDC Factor |   Test PPL\n",
            "------------------------------------------\n",
            "       1 sec |       1.0000 |     259.05\n",
            "      1 hour |       1.2965 |     258.89\n",
            "       1 day |       1.5198 |     258.65\n",
            "      1 year |       2.0412 |     259.09\n",
            "\n",
            "=======================================================\n",
            "üìà R√âSULTATS FINAUX:\n",
            "  Baseline (1 sec):  259.05\n",
            "  √Ä 1 an avec GDC:   259.09\n",
            "  D√©gradation r√©elle: +0.03 PPL (vs +14000 avant !)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Montage du Drive (si pas d√©j√† fait)\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2. Cr√©ation du dossier de sauvegarde\n",
        "save_dir = \"/content/drive/MyDrive/Projet_HWA_Final_SOTA\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# 3. Liste des fichiers pr√©cieux\n",
        "files_to_save = [\n",
        "    \"lstm_digital.pt\",       # Ton mod√®le digital\n",
        "    \"lstm_hwa.pt\",           # Ton mod√®le analogique\n",
        "    \"HWA_Training_AllInOne.ipynb\" # Ton code\n",
        "]\n",
        "\n",
        "print(\"üíæ Sauvegarde en cours...\")\n",
        "\n",
        "# Sauvegarde des fichiers mod√®les/code\n",
        "for f in files_to_save:\n",
        "    if os.path.exists(f):\n",
        "        shutil.copy(f, f\"{save_dir}/{f}\")\n",
        "        print(f\"   -> {f} sauvegard√©.\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è {f} introuvable (v√©rifie le nom).\")\n",
        "\n",
        "# 4. Sauvegarde des Logs (Preuve √©crite)\n",
        "# On √©crit tes r√©sultats finaux dans un fichier texte\n",
        "results_text = \"\"\"\n",
        "RESULTATS FINAUX - REPLICATION HWA LSTM\n",
        "=======================================\n",
        "Baseline Digital PPL : 330.96\n",
        "Baseline HWA PPL     : 257.59 (Am√©lioration par bruit r√©gularisateur)\n",
        "\n",
        "DRIFT ANALYSIS (1 AN) - Avec GDC Hooks\n",
        "---------------------------------------\n",
        "T=1s   : 259.05 PPL\n",
        "T=1an  : 259.09 PPL\n",
        "Delta  : +0.03 PPL (Stabilit√© Parfaite)\n",
        "\"\"\"\n",
        "with open(f\"{save_dir}/final_results.txt\", \"w\") as f:\n",
        "    f.write(results_text)\n",
        "print(\"   -> final_results.txt sauvegard√©.\")\n",
        "\n",
        "print(f\"‚úÖ TOUT EST S√âCURIS√â DANS : {save_dir}\")"
      ],
      "metadata": {
        "id": "sjVEuG4hlp_u",
        "outputId": "8a2b774b-7798-454e-f355-7c66a7ec9246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Sauvegarde en cours...\n",
            "   -> lstm_digital.pt sauvegard√©.\n",
            "   -> lstm_hwa.pt sauvegard√©.\n",
            "   ‚ö†Ô∏è HWA_Training_AllInOne.ipynb introuvable (v√©rifie le nom).\n",
            "   -> final_results.txt sauvegard√©.\n",
            "‚úÖ TOUT EST S√âCURIS√â DANS : /content/drive/MyDrive/Projet_HWA_Final_SOTA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drift Analysis - Approche correcte\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "print(\"\\nüìä DRIFT ANALYSIS (Approche Correcte)\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ntokens = len(corpus.dictionary)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "bptt = 35\n",
        "\n",
        "def eval_clean(model):\n",
        "    \"\"\"√âvaluation SANS bruit/drift - juste les poids appris\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # D√©sactiver compl√®tement le physics engine\n",
        "    for module in model.modules():\n",
        "        if hasattr(module, 'physics'):\n",
        "            module._orig_physics = module.physics\n",
        "            module.physics = None\n",
        "        if hasattr(module, 't_inference'):\n",
        "            module.t_inference = 0\n",
        "\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, test_data.size(0)-1, bptt):\n",
        "            seq_len = min(bptt, len(test_data)-1-i)\n",
        "            data = test_data[i:i+seq_len]\n",
        "            targets = test_data[i+1:i+1+seq_len].view(-1)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = tuple(h.detach() for h in hidden)\n",
        "            total_loss += len(data) * criterion(output.view(-1, ntokens), targets).item()\n",
        "\n",
        "    # Restaurer\n",
        "    for module in model.modules():\n",
        "        if hasattr(module, '_orig_physics'):\n",
        "            module.physics = module._orig_physics\n",
        "\n",
        "    return total_loss / (len(test_data)-1)\n",
        "\n",
        "# 1. √âvaluation du mod√®le HWA sans bruit (poids quantifi√©s propres)\n",
        "loss_hwa_clean = eval_clean(model_hwa)\n",
        "ppl_hwa_clean = math.exp(loss_hwa_clean)\n",
        "\n",
        "print(f\"Mod√®le HWA (sans bruit d'inf√©rence): {ppl_hwa_clean:.2f}\")\n",
        "print(f\"Mod√®le Digital baseline:             {330.96:.2f}\")\n",
        "print(f\"\\nAm√©lioration HWA: {330.96 - ppl_hwa_clean:.2f} points de PPL\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"üìù INTERPR√âTATION:\")\n",
        "print(\"\"\"\n",
        "Le HWA training a appris des poids ROBUSTES au bruit.\n",
        "√Ä l'inf√©rence sur hardware r√©el:\n",
        "- Les poids sont programm√©s UNE FOIS (avec bruit de prog.)\n",
        "- Le drift est D√âTERMINISTE et compens√© par GDC\n",
        "- Pas de re-sampling du bruit √† chaque forward pass\n",
        "\n",
        "Notre simulation r√©-√©chantillonne le bruit, ce qui n'est\n",
        "pas repr√©sentatif du hardware r√©el.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_NTgY5HfO4o",
        "outputId": "2588935d-38a3-4573-b4ba-c51133d3ec1b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä DRIFT ANALYSIS (Approche Correcte)\n",
            "=======================================================\n",
            "Mod√®le HWA (sans bruit d'inf√©rence): 257.59\n",
            "Mod√®le Digital baseline:             330.96\n",
            "\n",
            "Am√©lioration HWA: 73.37 points de PPL\n",
            "\n",
            "=======================================================\n",
            "üìù INTERPR√âTATION:\n",
            "\n",
            "Le HWA training a appris des poids ROBUSTES au bruit.\n",
            "√Ä l'inf√©rence sur hardware r√©el:\n",
            "- Les poids sont programm√©s UNE FOIS (avec bruit de prog.)\n",
            "- Le drift est D√âTERMINISTE et compens√© par GDC\n",
            "- Pas de re-sampling du bruit √† chaque forward pass\n",
            "\n",
            "Notre simulation r√©-√©chantillonne le bruit, ce qui n'est\n",
            "pas repr√©sentatif du hardware r√©el.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}